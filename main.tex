\documentclass[ams]{U-AizuGT}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{url}

\bibliographystyle{ieice}

\author{Tomoki Nakayama}
\studentid{s1270246}
\supervisor{Evgeny Pyshkin}
\title{Assessment of the Language Accent CNN Classifier for a Case of Asian Accents}
\begin{document}
\maketitle

\section*{Abstract}

\section{Introduction}
Presently, there are many conversational services provided by Google and Amazon products that include Google home, Alexa and other smart speakers, and smart assistants. However, those services still have much room for improvement to address the accent recognition, the latter is required to address the linguistic differences occurring in accented speech and affecting the accuracy of automatic speech recognition (ASR) models.
Conventional acoustic language models adapted to standard language corpora cannot be always directly applied to accent recognition.

A foreign language accent can be thought of as a combined effect of contact between the L1 and L2 phonological systems, where L1 is derived from the speaker's native language, while L2 refers to the second language \cite{Veranika:lang, Boula:the}. As reported in \cite{Rogerson:com}, ASR software is known to be highly accurate for recognizing the speech of native speakers, and significantly degrades for L2 speakers which might produce heavily accented speech. That's why accent-aware modeling has been reported as an efficient approach to improve misreading detection and diagnosis systems \cite{Jiang:tow, Algabri:mis}. Traditional acoustic language models adapted to standard language corpora cannot fully meet the recognition requirements for accented speech. Solving the problem of accented speech recognition by adding pronunciation samples to the dataset used for training is inappropriate because it increases processing time and introduces new noise, thus, affecting the recognition performance \cite{Deshpande:acc}. 

As the best known results so far have been achieved using mel-frequency cepstral coefficients (MFCC) that can be further enforced by time-frequency and energy features. The authors of \cite{Veranika：lang} reported the promising result of such a combination, while designing a CNN classifier trained and tested on English with Germanic, Italic and Slavic accents.
In this work, we make an effort to extend the experiments with up to eight Asian languages. Based on some reports about the difference in classification accuracy because of using different version of TensorFlow library, we decided to make the experiments agains two version, namely: tensorflow-macos 2.8.0 (since the version 2.8 was used in \cite{Veranika:lang}) and 2.11.0 (the latter being the latest version as of January 2023).\par
The remaining text organized as follows. Section~\ref{sec:related-work} describes the state-of-the-art approaches used in accent recognition and classification. Section~\ref{sec:related-work} presents the methods used in this study and describes the process workflow including data collection, CNN construction, its training and testing against the selected input features. Section ~\ref{sec:experiments-and-results} presents experimental results on hyperparameter selection, regularization, and different sets of features of the speech signal used by the CNN classifier. Section ~\ref{sec:conclusion} discusses the experimental results.

\section{Related Work}
\label{sec:related-work}
In previous studies, accent detection algorithms have been built using standard classification models and machine learning architectures including con- volutional neural networks (CNN) \cite{Singh:fea, Ensslin:dee, Ahamad:acc, Duong:dev},k-nearest neighbor (KNN) model \cite{Krishna:asy} \par 
Using MFCC was reported as one of the most successful approaces taking into account the physical and acoustic characteristics of the speech signal \cite{Singh:fea, Tverdokhleb:imp}. The authors in \cite{Singh:fea} suggested that further experiments are needed to identify promising cases for combining MFCC with other types of available features. Following this suggestion, in  \cite{Veranika:lang}, an exhaustive experimental study was conducted to investigate the issue. The experiments was resulted in concluding that mel-spectrograms on a linear scale demonstrated the high levels of accuracy with good efficiency.

\section{Methodology and Workflow}
\label{sec:methods}

The standard approach used in ASR assumes that the CNN deals with input that is actually a two-dimensional image representing the features of the speech signal \cite{Singh:fea}. The output value of the accent detection classifier is a probability distribution vector that attributes speech samples to a particular accent class (the latter corresponding to a specific language). \par
Accent information is being extracted from the speech using time-frequency transformation of the raw speech signal aimed to getting  the representative speech features such as MFCC \cite{Singh:fea}. After the features are extracted, machine learning methods are used to classify accents. 

\subsection{Data Collection}
For the experiment, speech samples were collected from the Speech Accent Archive \cite{George:spe} maintained by George Mason University, which is a crowdsourced collection of audio recordings of the following text passage:
\begin{quotation}{\it “Please call Stella. Ask her to bring these things with her from the store: Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob. We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.”}
\end{quotation}
Though the recording from Speech Accent Archive are not produced in the ideal acoustic conditions, this collection of speech data becomes popular, since it can be considered as a model closer to real-life situations compared to the specially constructed speech datasets. In the same time, the data availabe from Speech Accent Archive fit the important requirements for ASR-targeted dataset, including the support of speaker diversity, uniformity of material of the same content and context, variety of individual phonemes, phonemic balance, and others \cite{Veranika:lang}.

\subsection{Data Classes (L1 Languages)}
In this wore, we addressed a subset of 9 language groups. These groups were labeled according to L1 as Sino-Tibetan Languages(Chinese (CH), Thai (TH), Nepali (NE)), Indo-Iranian Languages (Persian(PE), Bengali (BE),Hindi (HI)) and others(Japanese(JA), Korean(KO) and English(EN)). The distribution of available recordings during the experimental period according to L1 classes was as shown in Table ~\ref{languages-chart}. Since the number of recordings with these accents are smaller compared to other language classes, it is important to  observe the possible trends within the larger groups.
%figure 1
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.35]{images/languages_chart.png}
    \caption{Distribution of audio recordings by classes (during experimental period).}
    \label{languages-chart}
\end{figure}
For classes with a large number of recordings, using all available cases usually does not improve recognition accuracy and computationally expensive. Therefore, for larger groups, the number of samples used was limited to 80 recordings selected randomly from the available collection.\par
For crowd-sourced collections, environment and recording equipment are not constant among speakers, which might resulting in large differences in sampling noise and recording volume \cite{Ahamad:acc}.
Following the recommendations from \cite{Veranika:lang, Ahamad:acc}, in order to reduce such possible differences, we used between speech recordings as linear distortion, before training the model, it is necessary to normalize, for example, using $z$-normalization of the obtained data within each speech recording as follows:
\\
\begin{equation}
x^{\prime}=\frac{(x-\mu)}{\sigma},
\end{equation}
\\
where $\mu$---mean value, $\sigma$---standard deviation.

\subsection{Fragments of Silence}

Similarly to the experiments arranged in \cite{Veranika:lang, Boula:the}, we investigated whether the pauses in speech need to be retained or removed from the input for further CNN model training and testing, now addressing the selection of Asian accent classes. Table ~\ref{silence} and Table ~\ref{silence-2} list  the results of this preliminary experimental stage. 

Since the accuracy is slightly higher for the test cases retaining the fragments of silence, for the subsequent main experiments, we used the audio files with all the pauses preserved.

\begin{table}[h]
    \centering
    \includegraphics[scale=0.43]{images/silence.png}
    \caption{Comparing classifiers with preserved or removed fragments of silence. (2.11.0)}
    \label{silence}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[scale=0.41]{images/silence_n.png}
    \caption{Comparing classifiers with preserved or removed fragments of silence. (2.8.0)}
    \label{silence-2}
\end{table}

\subsection{Feature Selection}
Features of the speech signal were extracted from the frames using applicable feature extraction methods, such as constructing a compact representation of the speech signal using a set of MFCCs derived from a real logarithmic cosine transform of the short-term spectrum expressed on the Mel frequency scale \cite{Zheng:com}.\par
The feature set is formed based on an \textbf{amplitude mel-spectrogram on a linear scale} and follows the studies \cite{Veranika:lang} and \cite{Singh:fea}. 

Thus, the frequency $f$ of the speech signal was transformed into a mel-spectrogram $M(f)$ as follows:
\\
\begin{equation}
M(f)=2595\log_{10}(1+\frac{f}{700}).
\end{equation}
\\

Then MFCC-based model \cite{Singh:fea} was extended by using a number of additional features as described in \cite{Veranika:lang}. These features include the following: 

\begin{itemize}
\setlength{\leftskip}{-4mm}
\item {\textbf{Spectral centroid} (SC) indicates where the center of mass of the spectrum is located. Perceptually, it has a strong relationship with the impression of sound brightness:
\\
\begin{equation}
C_t=\frac{\sum\nolimits_{n=1}^N M_t[n]*n}{\sum\nolimits_{n=1}^N M_t[n]},
\end{equation}
\\
$M_t[n]$ being the value of the frame signal spectrum t of the frequency interval $n$, Hz.
}
\item {\textbf{Spectral rolloff}(SR)\par
In the power spectrum, it is the value expressed by the value of frequency relative to the amplitude value of the energy of the percentage distribution given by the threshold value from the low frequency band to the whole, such as 95\% or 85\%.\par
Thus, the spectral rolloff is a frequency $R_t$ such as:
\\
\begin{equation}
\sum\nolimits_{n=1}^{R_t} M_t[n]=0.85*\sum\nolimits_{n=1}^N M_t[n],
\end{equation}
\\
where $M_t[n]$ is the value of the frame signal spectrum t of the frequency interval $n$, Hz. This value is used to determine vocalized sounds in speech, since the unvoiced sounds have a large proportion of the energy contained in the high frequency range of the spectrum.
}
\item {\textbf{Chromagram} is closely related to 12 different pitch classes. Specifically, chroma-based features, also called "pitch class profiles" are considered as powerful tools for analyzing music whose pitches are meaningfully classified (often into 12 categories) and whose tuning approximates an equal temperament scale. The main properties of chromaticity features are that they capture the harmonic and melodic characteristics of music and are robust to changes in timbre and instrumentation.}
\item {\textbf{Zero Crossing}(ZCR) is the point at which the sign of the mathematical function changes (e.g., from positive to negative) and is represented by the intercept (zero value) of the axis in the graph of the function:
\\
\begin{equation}
ZCR=\frac{1}{T-1}\sum\nolimits_{t=1}^{T-1} II\{S_{t-1}<0\},
\end{equation}\\
where $S_t$ is a signal of duration $t$, $II\{X\}$ is a characteristic function whose value is equal to $1$ if condition $X$ is satisfied and $0$, otherwise. For unvoiced speech, the ZCR-characteristic takes on higher values.
}
\item {\textbf{Root mean square}(RMS) is a standard measure representing the average signal strength:
\\
\begin{equation}
X_{RMS}=\sqrt{\frac{1}{N}\sum\nolimits_{n=0}^{N-1} |x[n]|^2}.
\end{equation}
\\
Calculating RMS directly from the audio recordings is faster because it does not require calculating STFT. However, using a spectrogram can give a more accurate representation of signal energy over time because its frames can be split into windows. Since the characteristics of the signal can be stored in an external file in advance (before training the model), decreasing the extraction time was not critical. That is why, in our case, to improve the signal representation accuracy, RMS was calculated based on the signal spectrogram.
}

\item {\textbf{Fundamental frequency}($f_0$) is defined as the lowest frequency of the periodic waveform.\par
To estimate the fundamental frequency ($f_0$), we used probabilistic YIN (pYIN) \cite{Mauch:pyi}, a modification of the YIN algorithm \cite{{De:yin}}. in the first stage of pYIN, the $f_0$ candidates and their probabilities are calculated by the YIN algorithm. In the second stage, the most likely $f_0$ sequences and voicing flags are estimated using Viterbi decoding.\par}

\end{itemize}

Thus, the first set of input features includes 30 audio descriptors: 13 MFCCs, 12 chroma coefficients, SC, SR, ZCR, RMS, and $f_0$.

\subsection{Classification Model}
The classification model for accent detection was built on the basis of the CNN used in \cite{Singh:fea}. The model consisted of two convolution layers with a ReLU activation function $ReLU(x)=max(0,x)$, $x$ being the value of the output neuron, and a two-dimensional filter. The first and second convolution layers consisted of 32 and 64 blocks, respectively. After each convolution layer, batch normalization and pooling were applied. The flat layer was followed by two dense layers of direct propagation.
The first dense layer consisted of 128 neurons and had a ReLU activation function; the second layer had the same number of neurons as the number of accents and used a softmax activation function:

\begin{equation}
softmax(z_i)=\frac{e^{z_i}}{\sum\nolimits_{j=1}^{C} e^{z_j}},
\end{equation}
\\
where $z_i$ is an element of the input vector of real numbers $z$, $C$ is the number of classes.

\begin{figure}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.22]{images/cnn_model.png}
    \caption{Classification process model for $N$ recognition classes.}
    \label{cnn-model}
\end{figure}

The figure \label{cnn-model} shows the cnn model's outline drawing.
We used standard Python libraries as tools for modeling, training, implementing, and visualizing CNNs. In particular, the accent classifier was implemented and trained using the Keras library, which provides a high-level interface to the Tensorflow computing platform. Classification quality metrics were computed using the Scikit-learn package. The Matplotlib library was used to visualize the experimental results.

\section{Experiments and Results}
\label{sec:experiments-and-results}
The first part of the experiments examined the architecture of the CNN across hyperparameter selection, regularization, and data augmentation. The second part focuses the experiments with various acoustic features fed into the input layer of the CNN model. All the experiments were performed on several classes of Asian accents.
\subsection{CNN Model Tuning and Data }
The kernel of a CNN convolution layer is the width and height of the filter mask. The most common filter sizes for convolution layers in machine learning are (3, 3) and (5, 5).\par
Several filter configurations were tried for classification of {EN, CH, PE, JA} (Mix Group) accents using linear amplitude mel spectrograms as input. The length of the input feature matrix used to represent the input data was 100; training was stopped when the change in recognition accuracy was less than 1\% within 10 epochs. Tables ~\ref{filter-size} and ~\ref{filter-size-2} show that Kernel Size (5, 5) and Pool Size (3, 3) also showed the best accuracy for both versions. Also, 2.8.0 had a wider range of accuracy and a much higher result of 0.8766.

\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.34]{images/filter_size.png}
    \caption{Results of using different filter sizes with mel-spectrograms. (2.11.0)}
    \label{filter-size}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.37]{images/filter_size_n.png}
    \caption{Results of using different filter sizes with mel-spectrograms. (2.8.0)}
    \label{filter-size-2}
\end{table}

In the data expansion phase, the maximum horizontal shift of 5\% and 10\% was tested on a subset of data containing speech recordings of foreign accent groups {PE, CH, JA} and speech files without foreign accents (EN). The MFCC and its alternating combination of fundamental frequency and spectral centroid were used as input data. The results are shown in Tables \ref{0.05-or-0.1} and \ref{0.05-or-0.1-2}.
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.38]{images/0.05_or_0.1.png}
    \caption{Classification results at different shift percentages for a set of languages of different language groups. (2.11.0)}
    \label{0.05-or-0.1}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.41]{images/0.05_or_0.1_n.png}
    \caption{Classification results at different shift percentages for a set of languages of different language groups. (2.8.0)}
    \label{0.05-or-0.1-2}
\end{table}

Based on these results, we hypothesized that a 5\% horizontal shift for version 2.11.0 and a 10\% or even higher value for version 2.8.0 would be better. Based on this hypothesis, we conducted experiments on the accents {CH, TH, NE} (Sino-Tibetan Languages). From the results in Tables ~\ref{horizontal-shift} and ~\ref{horizontal-shift-2}, we conclude that the optimal accuracy is reached when the horizontal shift is 5\% for both accents. This means that the hypothesis of version 2.8.0 has been rejected, but we can confirm the results of considerably higher accuracy and lower loss function at 20\% and 25\%.

\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.5]{images/horizontal_shift.png}
    \caption{Classification results at different shift percentages for a set of Romance languages. (2.11.0)}
    \label{horizontal-shift}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.5]{images/horizontal_shift_n.png}
    \caption{Classification results at different shift percentages for a set of Romance languages. (2.8.0)}
    \label{horizontal-shift-2}
\end{table}

\subsection{Input Acoustic Feature Sets}
\subsubsection{Dimension of Input}
Splitting the input features into large or small chunks can introduce bias. Large chunks can discover longer speech patterns (which are more likely to be accent-dependent), but the training set will be smaller, and training on high-dimensional data is naturally computationally expensive (and thus slower). In addition, selecting shorter fragments allows more input data to be used, but the information about the accents obtained from the feature vector fragments may be degraded.\par
Therefore, in order to find the optimal size of the input feature vectors, experiments were conducted by grouping the feature vectors into pieces ranging in size from 30 to 500 pieces per block. Tables ~\ref{feature-matrices1},~\ref{feature-matrices1-2},~\ref{feature-matrices2} and ~\ref{feature_matrices2-2} show the results.
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.43]{images/feature_matrices1.png}
    \caption{Classification results with different sizes of input matrices for Sino-Tibetan and Indo-Iranian languages (MFCC). (2.11.0)}
    \label{feature-matrices1}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.43]{images/feature_matrices1_n.png}
    \caption{Classification results with different sizes of input matrices for Sino-Tibetan and Indo-Iranian languages (MFCC). (2.8.0)}
    \label{feature-matrices1-2}
\end{table}

\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.4]{images/feature_matrices2.png}
    \caption{Classification results for different sizes of input matrices for mixed languages (MFCC). (2.11.0)}
    \label{feature-matrices2}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.4]{images/feature_matrices2_n.png}
    \caption{Classification results for different sizes of input matrices for mixed languages (MFCC). (2.8.0)}
    \label{feature-matrices2-2}
\end{table}
\clearpage
The results show that a feature matrix size between 30 and 150 is appropriate for both versions. Furthermore, if we pay attention to the difference between the two versions, we can see that the latest version is considerably less accurate at 200, but in the case of version 2.8.0, the values do not decrease significantly even at 200, so we can consider raising the upper limit of the range to 200.

\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.36]{images/mel_bands.png}
    \caption{Classification results for different sizes of input matrices for a set of Sino-Tibetan languages (mel-spectrograms). (2.11.0)}
    \label{mel-bands}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.34]{images/mel_bands_n.png}
    \caption{Classification results for different sizes of input matrices for a set of Sino-Tibetan languages (mel-spectrograms). (2.8.0)}
    \label{mel-bands-2}
\end{table}
Table ~\ref{mel-bands} and ~\ref{mel-bands-2} show the results of an experiment conducted on Sino-Tibetan accents, using the MFCC as input and varying the number of mel-bands used to represent the spectrogram.\par
A dropout of 0.1 was used during the experiment. The size of the filter in the convolution layer was (5, 5) and the size of the pooling layer was (3, 3). training was stopped when the recognition accuracy did not change by more than 1\% in 10 epochs.
As shown in Table 15 and 16, a mel-spectrogram with 64 frequency bands was found to be the most effective and was chosen as the input characteristic for recognition. On the other hand, using a 32-band mel-spectrogram requires less computation but results in a larger recognition error.\par
From the experimental results in Table 15 and 16, we can conclude that the optimal size of the input feature matrix is 50 vectors when the amplitude mel-spectrogram is used linearly.\par
When mel-bands is 30 and feature size is 200, it is the only size that is less than 0.5 compared to the other sizes, indicating that the above hypothesis is also true.
The difference between the two versions was shown to be that the latest version is less computationally intensive.

\subsubsection{Combining MFCC with additional features}
Next, we consider the case where MFCC is extended with some additional features, as proposed in \cite{Veranika:lang} and \cite{Singh:fea}. MFCC speech features are widely used for accent detection because they provide a compact yet informative speech signal with high classification accuracy \cite{Jiang:tow,Jiang:tow}. proposed that the accuracy can be further improved by adding additional information to the MFCC. However, arbitrarily adding a large number of input features is disadvantageous because the excessive information slows down the classifier's learning process and increases model overfitting due to noise. Therefore, it is important to select a limited number of appropriate representative features. Therefore, this study strives to find essential features for MFCC expansion that positively affect classification accuracy while maintaining the basic filter size in the hidden layer of the classifier. It is also worth noting that our feature selection is consistent with the Fisher criterion \cite{Longford:afa, Wu:fea, Sun:fea}, although we did not explicitly use it.\par
Learning was stopped when learning accuracy reached 90\% or 120 epochs for all accent sets, except for the cases {CH, TH, NE}(Sino-Tibetan Languages) where the learning process was terminated when 350 epochs were reached. The results obtained are shown in Tables ~\ref{features1},~\ref{features1-2},~\ref{features2} and ~\ref{features2-2}.\par
In the Sino-Tibetan language accent set {CH, TH, NE} in version 2.8.0, all results for MFCC combined with features are shown to be better than MFCC alone. Other than that, the addition of features to MFCC as features that tended to contribute to MFCC showed better accuracy overall. This is especially true for the chromogram and also for the combination of all features.\par
The difference between the two versions is that the latest version has higher overall accuracy. However, there were no other notable differences between the two versions.
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.35]{images/features1.png}
    \caption{Classification results using different types of input features for Sino-Tibetan and Indo-Iranian accents. (2.11.0)}
    \label{features1}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.35]{images/features1_n.png}
    \caption{Classification results using different types of input features for Sino-Tibetan and Indo-Iranian accents. (2.8.0)}
    \label{features1-2}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.35]{images/features2.png}
    \caption{ Classification results when using different types of input features for accents of mixed language groups. (2.11.0)}
    \label{features2}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.35]{images/features2_n.png}
    \caption{ Classification results when using different types of input features for accents of mixed language groups. (2.8.0)}
    \label{features2-2}
\end{table}
\clearpage
\subsubsection{Mel-Spectograms}
Linear-scale mel-amplitude spectrograms extracted from the speech signal can also be tried as input to the classifier \cite{Veranika:lang}. At the same time, according to the pre-set optimal parameters of the classifier, a filter with convolution layer size of (5, 5) and pooling layer size of (3, 3) is used, and the input feature matrix size is 30 elements. The number of epochs was limited to 60, and training was stopped when the change in recognition accuracy within 10 epochs was less than 1\%.\par
Tables ~\ref{mel-spectograms} and ~\ref{mel-spectograms-2} mention Mel-Spectograms on a linear scale showed high efficiency in recognizing foreign accents in English speech. \par
Mentioning about the version thing, as you can see at a glance, the latest version has a much higher accuracy. The average value was above 0.75 and never lower than 0.9. In comparison, the 2.8.0 version had an average value of 0.9122, a difference of about 0.05, and two values lower than 0.9.
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.4]{images/mel_spectograms.png}
    \caption{Accuracy and loss for trained Accent Classification Models. (2.11.0)}
    \label{mel-spectograms}
\end{table}
\begin{table}[h]
    \centering
    \includegraphics[keepaspectratio, scale=0.43]{images/mel_spectograms_n.png}
    \caption{Accuracy and loss for trained Accent Classification Models. (2.8.0)}
    \label{mel-spectograms-2}
\end{table}
\section{Conclusion}
\label{sec:conclusion}
The experimental results indicate that the hypothesis proposed in \cite{Singh:fea} that combining multiple features improves the system over MFCC alone is true for Asian languages. In addition, of the features used, a fairly high accuracy of 0.9 or better was obtained for both versions when the mel-spectogram was used. The system was found to be particularly effective not only in the area of speech processing, where its effectiveness has already been confirmed, but also in the recognition of accents in Asian languages.
In this experiment, we used two versions and compared the results of each, and found that there were significant differences between them. The latest version, 2.11.0, was shown to perform better than the older version, 2.8.0, overall in terms of accuracy, loss function results, and shorter training time. In addition, each version has some differences in the results, as confirmed in Table ~\ref{0.05-or-0.1} and Table ~\ref{0.05-or-0.1-2}, but the general trend was found to be the same.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% %%%%% references %%%%% %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{}
%1
\bibitem{Veranika:lang}Veranika Mikhailava, Evgeny Pyshkin, Mariia Lesnichaia, Natalia Bogach, Iurii Lezhenin, John Blake."Language Accent Detection with CNN Using Sparse Data from a Crowd-Sourced Speech Archive". 2022, 8;pp 17,18
%2
\bibitem{Boula:the}Boula de Mareüil, P.; Vieru, B. The Contribution of Prosody to the Perception of Foreign Accent. Phonetica 2006, 63, 247–267. doi: 10.1159/000097308.
%3
\bibitem{Rogerson:com}Rogerson-Revell, P.M. Computer-assisted pronunciation training (CAPT): Current issues and future directions. RELC J. 2021, 52, 189–205.
%4
\bibitem{Jiang:tow}Jiang, S.W.F.; Yan, B.C.; Lo, T.H.; Chao, F.A.; Chen, B. Towards robust mispronunciation detection and diagnosis for L2 English learners with accent-modulating methods. In Proceedings of the 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), Cartagena, Colombia, 13–17 December 2021; pp. 1065–1070.
%5
\bibitemP{Algabri:mis}Algabri, M.; Mathkour, H.; Alsulaiman, M.; Bencherif, M.A. Mispronunciation Detection and Diagnosis with Articulatory-Level Feedback Generation for Non-Native Arabic Speech. Mathematics 2022, 10, 2727. 
%6
\bibitem{Singh:fea}Singh, Y.; Pillay, A.; Jembere, E. Features of Speech Audio for Accent Recognition. In Proceedings of the 2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD), Durban, South Africa, 6–7 August 2020; pp. 1–6.
%7
\bibitem{Deshpande:acc}Deshpande, S.; Chikkerur, S.; Govindaraju, V. Accent classification in speech. In Proceedings of the Fourth IEEE Workshop on Automatic Identification Advanced Technologies (AutoID’05), Buffalo, NY, USA, 17–18 October 2005; pp. 139–143.
%26
\bibitem{Huang:ais}Huang, H.; Xiang, X.; Yang, Y.; Ma, R.; Qian, Y. Aispeech-sjtu accent identification system for the accented english speech recognition challenge. In Proceedings of the ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Toronto, ON, Canada, 6–11 June 2021; pp. 6254–6258.
%8
\bibitem{Tverdokhleb:imp}Tverdokhleb, E.; Dobrovolskyi, H.; Keberle, N.; Myronova, N. Implementation of accent recognition methods subsystem for eLearning systems. In Proceedings of the 2017 9th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS), Bucharest, Romania, 21–23 September 2017; Volume 2, pp. 1037–1041.
%9
\bibitem{Ensslin:dee}Ensslin, A.; Goorimoorthee, T.; Carleton, S.; Bulitko, V.; Poo Hernandez, S. Deep Learning for Speech Accent Detection in Video games. In Proceedings of the Thirteenth Artificial Intelligence and Interactive Digital Entertainment Conference, Salt Lake City, UT, USA, 5–9 October 2017; Volume 13.
%10
\bibitem{Bird:acc}Bird, J.; Wanner, E.; Ekárt, A.; Faria, D. Accent Classification in Human Speech Biometrics for Native and Non-native English Speakers. In Proceedings of the PErvasive Technologies Related to Assistive Environments (PETRA), Rhodes, Greece, 5–7 June 2019; pp. 554–560. 
%11
\bibitem{Ahamad:acc}Ahamad, A.; Anand, A.; Bhargava, P. AccentDB: A Database of Non-Native English Accents to Assist Neural Speech Recognition. arXiv 2020, arXiv:2005.07973.
%12
\bibitem{Duong:dev}Duong, Q.T.; Do, V.H. Development of Accent Recognition Systems for Vietnamese Speech. In Proceedings of the 2021 24th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA), Singapore, 18–20 November 2021; pp. 174–179.
%13
\bibitem{Krishna:asy}Krishna, G.R.; Krishnan, R.; Mittal, V.K. A system for automatic regional accent classification. In Proceedings of the 2020 IEEE 17th India Council International Conference (INDICON), New Delhi, India,10–13 December 2020; pp. 1–5.
%14
\bibitem{Lazaridis:ava}Lazaridis, A.; el Khoury, E.; Goldman, J.P.; Avanzi, M.; Marcel, S.; Garner, P.N. Swiss French Regional Accent Identification. In Proceedings of the Odyssey, Joensuu, Finland, 16–19 June 2014; pp. 106–111.
%15
\bibitem{Jiao:acc}Jiao, Y.; Tu, M.; Berisha, V.; Liss, J.M. Accent Identification by Combining Deep Neural Networks and Recurrent Neural Networks Trained on Long and Short Term Features. In Proceedings of the Interspeech, San Francisco, CA, USA, 8–12 September 2016; pp. 2388–2392.
%16
\bibitem{Weninger:dee}Weninger, F.; Sun, Y.; Park, J.; Willett, D.; Zhan, P. Deep Learning Based Mandarin Accent Identification for Accent Robust ASR. In Proceedings of the Interspeech, Graz, Austria, 15–19 September 2019; pp. 510–514.
%17
\bibitem{Is:tur}Is ̧ik, G.; Artuner, H. Turkish Dialect Recognition Using Acoustic and Phonotactic Features in Deep Learning Architectures. J. Inf. Technol. 2020, 13, 207–216.
%18
\bibitem{Kethireddy:mel}Kethireddy, R.; Kadiri, S.R.; Alku, P.; Gangashetty, S.V. Mel-Weighted Single Frequency Filtering Spectrogram for Dialect Identification. IEEE Access 2020, 8, 174871–174879. 
%19
\bibitem{George:spe}George Mason University. Speech Accent Archive, 2021. Available online: \url{https://accent.gmu.edu/} (accessed on 8 August 2022).
%20
\bibitem{Zheng:com}Zheng, F.; Zhang, G.; Song, Z. Comparison of different implementations of MFCC. J. Comput. Sci. Technol. 2001, 16, 582–589.
%21
\bibitem{Longford:afa}Longford, N.T. A fast scoring algorithm for maximum likelihood estimation in unbalanced mixed models with nested random effects. Biometrika 1987, 74, 817–827.
%22
\bibitem{Wu:fea}Wu, T.; Duchateau, J.; Martens, J.P.; Van Compernolle, D. Feature subset selection for improved native accent identification. Speech Commun. 2010, 52, 83–98.
%23
\bibitem{Sun:fea}Sun, L.; Wang, T.; Ding, W.; Xu, J.; Lin, Y. Feature selection using Fisher score and multilabel neighborhood rough sets for multilabel classification. Inf. Sci. 2021, 578, 887–912.
%24
\bibitem{Mauch:pyi}Mauch, Matthias, and Simon Dixon. “pYIN: A fundamental frequency estimator using probabilistic threshold distributions.” 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.
%25
\bibitem{De:yin}De Cheveigné, Alain, and Hideki Kawahara. “YIN, a fundamental frequency estimator for speech and music.” The Journal of the Acoustical Society of America 111.4 (2002): 1917-1930.

\end{thebibliography}

\end{document}