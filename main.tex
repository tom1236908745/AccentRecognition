\documentclass[ams]{U-AizuGT}
\documentclass{article}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{url} 

\bibliographystyle{ieice}

\author{Tomoki Nakayama}
\studentid{s1270246}
\supervisor{Evgeny Pyshkin}
\title{Assessment of the Language Accent CNN Classifier for a Case of Asian Accents}
\begin{document}
\maketitle

\section{Abstract}
Presently, there are many conversational services provided by Google and Amazon products that include Google home, Alexa and other smart speakers, and smart assistants. However, those services still have much room for improvement to address the accent recognition issues. Conventional acoustic language models adapted to standard language corpora cannot be directly applied to accent recognition .The study is based on the results of experiments on advancing a 2-level CNN classifier with mel-frequency cepstral coefficients (Singh et al., 2020) using a linear scale and additional input features such as spectral centroid, spectral roll-off, zero crossing and fundamental frequency. The results achieved for a selection of European accents are described in (Lesnichaia et al., 2022). 


\section{Introduction}
The accent of a foreign language can be thought of as the combined effect of contact between the L1 and L2 phonological systems, where L1 is derived from the speaker's native language and L2 refers to a second language [1].As reported in [2], the accuracy of automatic speech recognition (ASR) word processing software is high for native-speaking users and highly language, but it can be significantly degraded for L2 speakers, resulting in accented speech. In this context, accent-aware modeling has recently been reported as an efficient approach to improve misreading detection and diagnosis systems [3,4]. Traditional acoustic language models adapted to standard language corpora cannot meet the recognition requirements for accented speech. Solving the problem of accented speech recognition by adding pronunciation samples to the dataset used for training is inappropriate because it increases processing time and introduces new noise that degrades performance [8]. On the other hand, automatic foreign language accent recognition (speaker L1 detection based on L2 samples) can improve the robustness of ASR-based software and computer-assisted pronunciation training (CAPT) systems. Accent detection can help overcome undesirable variability in speaker-independent speech recognition models [5-7].

\section{Methods and Experimental Setting}

\subsection{existing results}
By previous reserch, Accent detection algorithms have been built using standard classification models and machine learning architectures including con- volutional neural networks (CNN) [5,11,16,21], feedforward neural networks (FFNN) [10], hidden Markov model (HMM) [13], k-nearest neighbor (KNN) model [22], Gaussian mix- ture model (GMM) [23,24], long short-term memory (LSTM) and bidirectional LSTM (bLSTM) [25,26], random forest, and support vector machine (SVM) [13,22,24,27,28].They also stated that the use of MFCC as an input characteristic, taking into account the physical and acoustic characteristics of the speech signal, is one of the most common approaches in ASR solutions [5,10,13,17]. The authors in [5] also suggested that further experiments are needed to identify promising cases for combining MFCC with other types of available features.

\subsection{Methods}
The standard approach used in ASR assumes that the CNN deals with input that is actually a two-dimensional image representing the features of the speech signal [5]. The output value of the accent detection classifier is a probability distribution vector that attributes speech samples to a particular accent class (class corresponds to language). \par
To elaborate on the process of producing the output, accent information is first obtained by applying a specific time-frequency transformation to the raw speech signal and using methods to compute more sophisticated speech parameters such as MFCC [5]. After the features are extracted, machine learning methods are used to classify accents [10,13,30]. This method is mainly determined by considering three main topics at all stages of automatic accent detection: data set, feature selection, and machine learning model design.
\subsubsection{data collection}
For the experiment, speech samples were collected from the Speech Accent Archive [29] maintained by George Mason University, which is a crowdsourced collection of audio recordings of sentences such as
\begin{quotation}{\it “Please call Stella. Ask her to bring these things with her from the store: Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob. We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.”}
\end{quotation}
This data was selected based on the main aptitude requirements for ASR, taking into account the following conditions.
\begin{itemize}
\item {Speaker diversity: the ability to adequately represent different pronunciation variations.}
\item {Uniformity of material of the same content and context.}
\item {Not too frequent occurrence of individual phonemes, phonemic balance.}
\item {The presence of semantic loading of sentences to avoid semantic elements that may affect pronunciation [17].}
\item {To deal with speech segments rather than independent words.}
\end{itemize}


\subsubsection{Data Classes (L1 Languages)}
We used a subset of 9 language groups. These groups were labeled according to L1 as Sino-Tibetan Languages(Chinese (CH), Thai (TH), Nepali (NE)), Indo-Iranian Languages (Persian(PE), Bengali (BE),Hindi (HI)) and others(Japanese(JA), Korean(KO) and English(EN)). The distribution of available recordings during the experimental period according to L1 classes was as shown in Figure 1.
%Figure 1
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.35]{images/languages_chart.png}
    \caption{Distribution of audio recordings by classes (during experimental period).}
    
\end{figure}
For classes with a large number of recordings, using all available cases does not improve recognition accuracy much and may be computationally expensive. Therefore, for large groups, the number of samples used was limited to 80 recordings.

The disadvantage of using crowdsourced datasets is that the recording environment and recording equipment are not constant among speakers, resulting in large differences in sampling noise and recording volume [17]. Therefore, in order to reduce such differences between speech recordings as linear distortion,before training the model, it is necessary to normalize the obtained data within each speech recording, for example, using z-normalization (using z-score):
\\
\begin{equation}
x^{\prime}=\frac{(x-\mu)}{\sigma},
\end{equation}
\\
where $\mu$---mean value, $\sigma$---standard deviation.
\subsubsection{Fragments of Silence}
In addition, when dealing with speech data, it is necessary to consider whether silent fragments (pauses) need to be retained or removed from the input. Table 2 summarizes the results. In the experiments that follow, the audio files with all pauses preserved were used as is. The same results were obtained in the study of [1] regarding this.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.33]{images/silence.png}
    \caption{Comparing classifiers with preserved or removed fragments of silence.}
\end{figure}
\subsection{Feature Selection}
Features of the speech signal were extracted from the frames using applicable feature extraction methods, such as constructing a compact representation of the speech signal using a set of Mel Frequency Cepstrum Coefficients (MFCC) derived from a real logarithmic cosine transform of the short-term spectrum expressed on the Mel frequency scale [30].\\
The feature set was formed based on an \textbf{amplitude melspectrogram on a linear scale}. The frequency f of the speech signal was transformed into a melspectrogram M( f ) as follows:
\\
\begin{equation}
M(f)=2595\log_{10}(1+\frac{f}{700}).
\end{equation}
\\

As proposed in [5], combining additional features with MFCC can contribute to further recognition accuracy. In this study, several representative experiments were arranged to test this hypothesis: six additional features were used to extend the MFCC-based model.

\begin{itemize}
\setlength{\leftskip}{-4mm}
\listparindent = 4mm
\item {\textbf{Spectral centroid} (SC) represents “center of mass” of the input sound, which formally corresponds to the frequency at which the energy of the spectrum is concentrated:
\\
\begin{equation}
C_t=\frac{\sum\nolimits_{n=1}^N M_t[n]*n}{\sum\nolimits_{n=1}^N M_t[n]},
\end{equation}
\\
$M_t$[n] being the value of the frame signal spectrum t of the frequency interval n, Hz.
}
\item {\textbf{Spectral rolloff}(SR) is a measure of the asymmetry of the spectral shape of the signal. It represents the frequency $R_t$, such as a given percentage (usually 85\%) of the total
energy of the spectrum that lies below Rt. In order to calculate this value, one needs to find the proportion of frames in the signal power spectrum, where a given percentage of power falls on lower frequencies. Thus, the spectral rolloff is a frequency $R_t$ such as:
\\
\begin{equation}
\sum\nolimits_{n=1}^{R_t} M_t[n]=0.85*\sum\nolimits_{n=1}^N M_t[n],
\end{equation}
\\
where $M_t$[n] is the value of the frame signal spectrum t of the frequency interval n, Hz. This value is used to determine vocalized sounds in speech, since the unvoiced sounds have a large proportion of the energy contained in the high frequency range of the spectrum.
}
\item {\textbf{Chromagram}is usually a 12-dimensional feature vector representing the amount of energy for each of the signal’s height classes (such as C, C\#, D, D\#, E, etc.).}
\item {\textbf{Zero Crossing}(ZCR) represents the number of signal sign changes within a segment. The ZCR feature can be helpful in describing the signal noisiness:
\\
\begin{equation}
ZCR=\frac{1}{T-1}\sum\nolimits_{t=1}^{T-1} II(\{S_{t-1}<0\},
\end{equation}\\
where St is a signal of duration t, II{X} is a characteristic function whose value is equal to 1 if condition X is satisfied and 0, otherwise. For unvoiced speech, the ZCR characteristic takes on higher values.
}
\item {\textbf{Root mean square}(RMS) is a standard measure representing the average signal strength:
\\
\begin{equation}
X_{RMS}=\sqrt{\frac{1}{N}\sum\nolimits_{n=0}^{N-1} |x[n]|^2}.
\end{equation}
\\
}
\end{itemize}



%%%%% references %%%%% 
\begin{thebibliography}{99}
\bibitem{}xxxx
\url{https://}
\end{thebibliography}

%\section*{Acknowledgement}
\bibliography{biblist}

\end{document}